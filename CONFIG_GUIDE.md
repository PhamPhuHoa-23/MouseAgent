# ğŸ“„ HÆ°á»›ng dáº«n Config cho Pre-training

## ğŸ¯ Cáº¥u trÃºc Config Files

### 1. **Main Workflow Config**: `dino_workflow_config.yaml`
Äiá»u khiá»ƒn toÃ n bá»™ pipeline (data collection â†’ pre-training â†’ RL fine-tuning):

```yaml
data_collection:
  env: NormalTrain                    # Unity environment  
  episodes: 200                       # Sá»‘ episodes thu tháº­p
  steps_per_episode: 500              # Steps per episode
  output_dir: ./dataset_mouse_dino    # Output directory
  additional_envs:                    # Thu tháº­p thÃªm tá»« environments khÃ¡c
    - FogTrain
    - RandomTrain
  train_val_split: true               # TÃ¡ch train/val

pretraining:
  epochs: 50                          # Sá»‘ epochs pre-train
  batch_size: 32                      # Batch size (tuá»³ GPU)
  output_dir: ./dino_checkpoints      # NÆ¡i lÆ°u checkpoints
  use_wandb: false                    # Sá»­ dá»¥ng W&B logging
  model:
    img_size: 224                     # Input image size
    patch_size: 16                    # ViT patch size
    embed_dim: 384                    # Embedding dimension
    depth: 6                          # Sá»‘ transformer layers
    num_heads: 6                      # Attention heads

finetuning:
  architectures:
    - dino_encoder                    # Encoder architecture
  runs_per_network: 3                 # Sá»‘ runs per architecture
  max_steps: 50000                    # Max RL training steps
  output_dir: ./dino_rl_results       # RL results directory
  unfreeze_layers: 2                  # Layers Ä‘á»ƒ fine-tune
```

### 2. **DINO Pre-training Config**: `dino_config.yaml` 
Chi tiáº¿t config cho self-supervised pre-training (tá»± Ä‘á»™ng táº¡o):

```yaml
model:
  img_size: 224                       # Image size
  patch_size: 16                      # Patch size cho ViT
  embed_dim: 384                      # Embedding dimension
  depth: 6                            # Transformer depth
  num_heads: 6                        # Multi-head attention

head:
  out_dim: 65536                      # Output dimension
  bottleneck_dim: 256                 # Bottleneck dimension

loss:
  out_dim: 65536                      # Loss output dim
  teacher_temp: 0.07                  # Teacher temperature
  student_temp: 0.1                   # Student temperature

optimizer:
  lr: 0.0005                          # Learning rate
  weight_decay: 0.04                  # Weight decay

training:
  epochs: 50                          # Training epochs
  batch_size: 32                      # Batch size
  teacher_momentum: 0.996             # EMA momentum
```

### 3. **ML-Agents Config**: `Config/nature.yaml`, etc.
Cho RL fine-tuning:

```yaml
behaviors:
  My Behavior:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128
      learning_rate: 0.0003
      # ... other PPO hyperparameters
    network_settings:
      normalize: false
      hidden_units: 256
      vis_encode_type: nature_cnn     # Sáº½ Ä‘Æ°á»£c thay báº±ng custom encoder
```

## ğŸ”§ CÃ¡ch tÃ¹y chá»‰nh Config

### 1. **TÃ¹y chá»‰nh Pre-training Parameters**

Sá»­a `dino_workflow_config.yaml`:

```yaml
pretraining:
  epochs: 100              # TÄƒng epochs cho better quality
  batch_size: 64           # TÄƒng batch size náº¿u cÃ³ GPU máº¡nh
  model:
    embed_dim: 768         # TÄƒng model size
    depth: 12              # Model sÃ¢u hÆ¡n
    num_heads: 12          # Nhiá»u attention heads hÆ¡n
```

### 2. **Äiá»u chá»‰nh theo GPU Memory**

| GPU Memory | Batch Size | Model Size | Embed Dim | Depth |
|------------|------------|------------|-----------|-------|
| 8GB        | 16-32      | Small      | 192-384   | 3-6   |
| 16GB       | 32-64      | Medium     | 384-512   | 6-9   |
| 24GB+      | 64-128     | Large      | 512-768   | 9-12  |

### 3. **TÃ¹y chá»‰nh Data Collection**

```yaml
data_collection:
  episodes: 500            # Nhiá»u data hÆ¡n
  steps_per_episode: 1000  # Episodes dÃ i hÆ¡n
  additional_envs:         # Thu tháº­p tá»« nhiá»u environments
    - FogTrain
    - RandomTrain
    - CustomEnv
```

### 4. **Fine-tuning Settings**

```yaml
finetuning:
  architectures:
    - dino_encoder         # Architecture chÃ­nh
    - dino_resnet_hybrid   # ThÃªm architectures khÃ¡c Ä‘á»ƒ so sÃ¡nh
  runs_per_network: 5      # Nhiá»u runs Ä‘á»ƒ cÃ³ káº¿t quáº£ stable
  unfreeze_layers: 4       # Fine-tune nhiá»u layers hÆ¡n
```

## ğŸ“ Vá»‹ trÃ­ cÃ¡c Config Files

```
mouse_vs_ai_windows/
â”œâ”€â”€ dino_workflow_config.yaml           # â­ MAIN CONFIG
â”œâ”€â”€ dino_config.yaml                    # Auto-generated tá»« main config
â”œâ”€â”€ resnet50_workflow_config.yaml       # Config cho ResNet50
â”œâ”€â”€ Config/
â”‚   â”œâ”€â”€ nature.yaml                     # Base ML-Agents config
â”‚   â”œâ”€â”€ dinov3_train.yaml              # DINOv3 specific
â”‚   â”œâ”€â”€ resnet50_config.yaml           # ResNet50 specific
â”‚   â””â”€â”€ ...
â””â”€â”€ config_examples/
    â”œâ”€â”€ dino_finetuning.yaml           # Auto-generated finetuning config
    â””â”€â”€ resnet50_finetuning.yaml       # Auto-generated ResNet config
```

## ğŸš€ CÃ¡ch sá»­ dá»¥ng Config

### 1. **Sá»­ dá»¥ng default config**:
```bash
# Tá»± Ä‘á»™ng táº¡o dino_workflow_config.yaml vá»›i default values
python dino_workflow.py --step all
```

### 2. **Sá»­ dá»¥ng custom config**:
```bash
# Táº¡o custom config file
cp dino_workflow_config.yaml my_custom_config.yaml
# Sá»­a parameters trong my_custom_config.yaml

# Cháº¡y vá»›i custom config
python dino_workflow.py --config my_custom_config.yaml --step all
```

### 3. **Override config parameters**:
```bash
# Sá»­ dá»¥ng script riÃªng vá»›i custom parameters
python run_pretrain_only.py --epochs 100 --batch-size 64
```

## ğŸ¯ Config Templates

### **Small/Fast Config** (Testing):
```yaml
pretraining:
  epochs: 20
  batch_size: 16
  model:
    embed_dim: 192
    depth: 3
    num_heads: 3
data_collection:
  episodes: 50
```

### **Medium Config** (Balanced):
```yaml
pretraining:
  epochs: 50
  batch_size: 32
  model:
    embed_dim: 384
    depth: 6
    num_heads: 6
data_collection:
  episodes: 200
```

### **Large Config** (Best Quality):
```yaml
pretraining:
  epochs: 100
  batch_size: 64
  model:
    embed_dim: 768
    depth: 12
    num_heads: 12
data_collection:
  episodes: 500
```

## ğŸ’¡ Tips cho Config

### 1. **Start Small, Scale Up**:
- Báº¯t Ä‘áº§u vá»›i small config Ä‘á»ƒ test pipeline
- TÄƒng dáº§n parameters khi Ä‘Ã£ á»•n Ä‘á»‹nh

### 2. **Monitor GPU Usage**:
```bash
# Theo dÃµi GPU memory
nvidia-smi -l 5

# Giáº£m batch_size náº¿u out of memory
```

### 3. **Backup configs**:
```bash
# Backup config trÆ°á»›c khi modify
cp dino_workflow_config.yaml dino_workflow_config.yaml.backup
```

### 4. **Version Control**:
- Commit config changes vÃ o git
- Tag cÃ¡c config versions quan trá»ng

## ğŸ” Debug Config Issues

### Config file khÃ´ng tá»“n táº¡i:
```bash
# Táº¡o default config
python -c "
from dino_workflow import DINOWorkflow
workflow = DINOWorkflow()
print('âœ… Default config created!')
"
```

### Config syntax errors:
```bash
# Validate YAML syntax
python -c "
import yaml
with open('dino_workflow_config.yaml') as f:
    config = yaml.safe_load(f)
print('âœ… Config syntax valid!')
"
```

### Wrong parameters:
- Kiá»ƒm tra logs khi cháº¡y training
- So sÃ¡nh vá»›i working configs
- Test vá»›i minimal config trÆ°á»›c

**ğŸ¯ Main Config File: `dino_workflow_config.yaml` - ÄÃ¢y lÃ  file quan trá»ng nháº¥t cáº§n tÃ¹y chá»‰nh!**

