# # backbone:
# #   arch: resnet18
# #   output_dim: 1024
# #   pretrained: true
# # head:
# #   bottleneck_dim: 256
# #   out_dim: 1024
# # loss:
# #   out_dim: 1024
# #   student_temp: 0.1
# #   teacher_temp: 0.04
# # optimizer:
# #   backbone_lr: 0.0005
# #   lr: 0.001
# #   weight_decay: 0.0001
# # training:
# #   batch_size: 16
# #   epochs: 30
# #   teacher_momentum: 0.996
# # Enhanced DINO ResNet Configuration
# # Optimized for object-centric attention with multi-crop strategy

# ResNet Backbone Configuration
backbone:
  input_channels: 1
  arch: resnet34              # resnet18, resnet34, resnet50
  pretrained: true            # Use ImageNet pretrained weights
  output_dim: 1024             # Feature dimension (512 for ResNet18/34, 2048 for ResNet50)

# DINO Projection Head Configuration  
head:
  out_dim: 1024              # Output dimension for DINO features
  bottleneck_dim: 256        # Bottleneck dimension in MLP
  nlayers: 3                 # Number of layers in projection head

# Multi-Crop Strategy (KEY for object attention!)
crops:
  # Global crops - capture full image context
  global_crops_number: 2     # Number of global crops per image
  global_size: 224           # Resolution of global crops
  global_scale: [0.6, 1.0]   # Scale range (0.6-1.0 reduces background)
  
  # Local crops - focus on object parts (CRITICAL!)
  local_crops_number: 6      # Number of local crops per image
  local_size: 96             # Resolution of local crops
  local_scale: [0.15, 0.4]   # Scale range (small patches focus on objects)

# DINO Loss Configuration
loss:
  out_dim: 1024              # Must match head.out_dim
  
  # Teacher temperature scheduling
  warmup_teacher_temp: 0.04  # Initial teacher temperature
  teacher_temp: 0.04         # Final teacher temperature  
  warmup_teacher_temp_epochs: 30  # Epochs for temperature warmup
  
  # Student temperature (fixed)
  student_temp: 0.1          # Student temperature (higher = softer)
  
  # Center momentum for teacher centering
  center_momentum: 0.9       # EMA momentum for updating center

# Optimizer Configuration
optimizer:
  lr: 0.001                  # Learning rate for new layers (projection + head)
  backbone_lr: 0.0001        # Lower LR for pretrained backbone
  weight_decay: 0.0001       # L2 regularization

# Training Configuration
training:
  epochs: 30                # Total training epochs
  batch_size: 64             # Batch size (adjust based on GPU memory)
  
  # Teacher EMA updates
  teacher_momentum: 0.996    # EMA momentum for teacher updates
  
  # Learning rate warmup
  warmup_epochs: 0          # Epochs for LR warmup

# Data Augmentation (built into transforms)
augmentation:
  # Color augmentations
  color_jitter:
    brightness: 0.4
    contrast: 0.4
    saturation: 0.2
    hue: 0.1
  
  # Other augmentations
  random_grayscale_prob: 0.2  # Probability of grayscale conversion
  horizontal_flip_prob: 0.5   # Probability of horizontal flip
  gaussian_blur:
    kernel_size: 23
    sigma: [0.1, 2.0]

# System Configuration
system:
  num_workers: 4             # DataLoader workers
  pin_memory: true           # Pin memory for faster GPU transfer
  mixed_precision: false     # Use automatic mixed precision (experimental)

# Logging Configuration
logging:
  wandb_project: "mouse-dino-resnet-enhanced"
  log_interval: 20           # Log every N batches
  save_interval: 20          # Save checkpoint every N epochs
  
# Memory Optimization
memory:
  gradient_checkpointing: false  # Use gradient checkpointing (saves memory)
  empty_cache_interval: 50       # Clear CUDA cache every N batches

# Validation (optional)
validation:
  enabled: false             # Enable validation during training
  data_dir: null             # Path to validation data
  interval: 10               # Validate every N epochs

# Experimental Features
experimental:
  # Attention regularization
  attention_regularization: false
  attention_reg_weight: 0.01
  
  # Multi-scale training
  multi_scale_training: false
  scales: [0.8, 1.0, 1.2]
  
  # Object-aware cropping (requires object detection labels)
  object_aware_cropping: false
  object_crop_prob: 0.3

# Hardware-Specific Configs

# For RTX 4060 8GB
rtx_4060_8gb:
  backbone:
    arch: resnet18           # Lighter architecture
  training:
    batch_size: 16           # Smaller batch size
  crops:
    local_crops_number: 6    # Fewer local crops
  system:
    num_workers: 4           # Fewer workers

# For RTX 3080 10GB  
rtx_3080_10gb:
  training:
    batch_size: 48           # Larger batch size
  crops:
    local_crops_number: 8    # More local crops

# For RTX 4090 24GB
rtx_4090_24gb:
  training:
    batch_size: 64           # Large batch size
  crops:
    local_crops_number: 10   # Many local crops
    global_crops_number: 4   # More global crops

# Quick Configs for Different Scenarios

# Fast training (for testing)
quick_test:
  training:
    epochs: 20
    batch_size: 16
  crops:
    local_crops_number: 4

# High quality training (for production)
high_quality:
  training:
    epochs: 200
    batch_size: 32
  crops:
    local_crops_number: 8
    global_crops_number: 4
  head:
    nlayers: 4
    bottleneck_dim: 512

# Object-focused training (maximum object attention)
object_focused:
  crops:
    global_scale: [0.7, 1.0]     # Even less background
    local_scale: [0.1, 0.3]      # Smaller local crops
    local_crops_number: 10       # More local crops
  loss:
    teacher_temp: 0.03           # Sharper teacher predictions
# Simple Object-Focused DINO Config
# Based on working config + minimal changes for object attention

# ResNet Backbone Configuration
# backbone:
#   arch: resnet34              # Keep proven architecture
#   pretrained: true            # Use ImageNet pretrained weights
#   output_dim: 1024           # Feature dimension

# # DINO Projection Head Configuration  
# head:
#   out_dim: 1024              # Output dimension for DINO features
#   bottleneck_dim: 256        # Bottleneck dimension in MLP
#   nlayers: 3                 # Number of layers in projection head

# # OBJECT-FOCUSED Multi-Crop Strategy - KEY CHANGES HERE!
# crops:
#   # Global crops - less background
#   global_crops_number: 2     # Number of global crops per image
#   global_size: 224           # Resolution of global crops
#   global_scale: [0.8, 1.0]   # CHANGED: less background (was [0.6, 1.0])
  
#   # Local crops - MUCH smaller for objects
#   local_crops_number: 10     # CHANGED: more attempts (was 6)
#   local_size: 96             # Resolution of local crops
#   local_scale: [0.05, 0.2]   # CHANGED: tiny crops for objects (was [0.15, 0.4])

# # DINO Loss Configuration
# loss:
#   out_dim: 1024              # Must match head.out_dim
  
#   # Teacher temperature scheduling - sharper for objects
#   warmup_teacher_temp: 0.03  # CHANGED: sharper (was 0.04)
#   teacher_temp: 0.03         # CHANGED: sharper (was 0.04)
#   warmup_teacher_temp_epochs: 30
  
#   # Student temperature
#   student_temp: 0.1          # Keep same
  
#   # Center momentum
#   center_momentum: 0.9       # EMA momentum for updating center

# # Optimizer Configuration
# optimizer:
#   lr: 0.001                  # Learning rate for new layers
#   backbone_lr: 0.0001        # Lower LR for pretrained backbone
#   weight_decay: 0.0001       # L2 regularization

# # Training Configuration
# training:
#   epochs: 30                 # Keep same for testing
#   batch_size: 16             # Keep same for RTX 4060
  
#   # Teacher EMA updates
#   teacher_momentum: 0.996    # EMA momentum for teacher updates
  
#   # Learning rate warmup
#   warmup_epochs: 0           # Keep simple

# # Data Augmentation - enhanced for object visibility
# augmentation:
#   # Color augmentations - boost contrast for objects
#   color_jitter:
#     brightness: 0.3          # CHANGED: less extreme (was 0.4)
#     contrast: 0.5            # CHANGED: higher for objects (was 0.4)
#     saturation: 0.2
#     hue: 0.1
  
#   # Other augmentations
#   random_grayscale_prob: 0.1 # CHANGED: less grayscale (was 0.2)
#   horizontal_flip_prob: 0.5
#   gaussian_blur:
#     kernel_size: 23
#     sigma: [0.1, 2.0]

# # System Configuration
# system:
#   num_workers: 4             # DataLoader workers
#   pin_memory: true           # Pin memory for faster GPU transfer
#   mixed_precision: false     # Keep simple

# # Logging Configuration
# logging:
#   wandb_project: "mouse-dino-object-focused"
#   log_interval: 20           # Log every N batches
#   save_interval: 10          # CHANGED: save more often (was 20)
  
# # Memory Optimization
# memory:
#   gradient_checkpointing: false
#   empty_cache_interval: 50

# # Validation
# validation:
#   enabled: false             # Keep simple
#   data_dir: null
#   interval: 10

# # Experimental Features - keep minimal
# experimental:
#   attention_regularization: false
#   attention_reg_weight: 0.01
#   multi_scale_training: false
#   scales: [0.8, 1.0, 1.2]
#   object_aware_cropping: false
#   object_crop_prob: 0.3