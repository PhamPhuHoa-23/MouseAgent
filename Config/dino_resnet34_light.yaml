behaviors:
  My Behavior:
    trainer_type: ppo
    init_path: results\dino_resnet34_light\My Behavior\My Behavior-159954.pt
    hyperparameters:
      batch_size: 64         # Reduced from 128
      buffer_size: 4096       # Reduced from 2048
      learning_rate: 0.0001
      beta: 0.01
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 2
      vis_encode_type: simple  # Will be replaced by DINO ResNet34
    # action_noise:
    #   type: gaussian
    #   sigma: 0.2  
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    checkpoint_interval: 20000
    keep_checkpoints: 5
    max_steps: 2000000
    summary_freq: 1000
    threaded: true
# behaviors:
#   My Behavior:
#     trainer_type: sac
#     init_path: results\dino_resnet34_light\My Behavior\My Behavior-26947.pt  # Path to pre-trained DINO ResNet34
#     hyperparameters:
#       batch_size: 64         # Reduced from 128, suitable for continuous actions
#       buffer_size: 512     # Increased for SAC to learn from more experiences, supports continuous actions
#       learning_rate: 0.0003  # Kept same as PPO, adjustable if unstable
#       init_entcoef: 0.5      # Initial entropy coefficient for exploration, adjustable (0.5-1.0)
#       tau: 0.005             # Soft update rate for target networks, typical for SAC
#       steps_per_update: 1    # Ratio of agent steps to policy updates, kept low for stability
#       learning_rate_schedule: constant  # SAC typically uses constant learning rate
#     network_settings:
#       normalize: false
#       hidden_units: 256      # Increased for better feature representation
#       num_layers: 2          # Kept same, can increase to 3 if needed
#       vis_encode_type: resnet # Use ResNet34 pre-trained (ensure ONNX/checkpoint compatibility)
#     reward_signals:
#       extrinsic:
#         gamma: 0.99          # Discount factor, kept same as PPO
#         strength: 1.0        # Kept same, scales the reward signal
#     checkpoint_interval: 20000  # Kept same, save checkpoint every 20k steps
#     keep_checkpoints: 5         # Kept same, max 5 checkpoints
#     max_steps: 2000000          # Kept same, total steps for training
#     summary_freq: 1000          # Kept same, summary every 1k steps
#     threaded: true              # Kept same, enables multi-threading for faster training